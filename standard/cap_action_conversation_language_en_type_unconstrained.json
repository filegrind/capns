{
  "urn": {
    "tags": {
      "action": "conversation",
      "language": "en",
      "type": "unconstrained"
    }
  },
  "command": "llm_inference",
  "cap_description": "Natural conversation and chat responses in English",
  "metadata": {},
  "accepts_stdin": true,
  "arguments": {
    "required": [
      {
        "name": "prompt",
        "arg_type": "string",
        "arg_description": "User's conversational input",
        "cli_flag": "--prompt",
        "position": 0,
        "validation": {
          "min_length": 1,
          "max_length": 32768
        }
      }
    ],
    "optional": [
      {
        "name": "context",
        "arg_type": "string",
        "arg_description": "Conversation context and history",
        "cli_flag": "--context",
        "validation": {
          "max_length": 65536
        }
      },
      {
        "name": "max_tokens",
        "arg_type": "integer",
        "arg_description": "Maximum tokens to generate",
        "cli_flag": "--max-tokens",
        "default_value": 2000,
        "validation": {
          "min": 1,
          "max": 8192
        }
      },
      {
        "name": "temperature",
        "arg_type": "number",
        "arg_description": "Sampling temperature for creativity vs consistency",
        "cli_flag": "--temperature",
        "default_value": 0.7,
        "validation": {
          "min": 0,
          "max": 2
        }
      },
      {
        "name": "system_prompt",
        "arg_type": "string",
        "arg_description": "System instructions for conversation behavior",
        "cli_flag": "--system-prompt",
        "validation": {
          "max_length": 8192
        }
      },
      {
        "name": "model_spec",
        "arg_type": "string",
        "arg_description": "Model spec to use for conversation",
        "cli_flag": "--model-spec",
        "default_value": "hf:MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF?include=*Q8_0*,*.json&exclude=*IQ1*,*IQ2*,*IQ3*,*Q2*,*Q3*",
        "validation": {
          "pattern": ".*:.*",
          "min_length": 4,
          "max_length": 512
        }
      }
    ]
  },
  "output": {
    "output_type": "object",
    "output_description": "Generated conversational response with context information",
    "content_type": "application/json",
    "schema": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "response": {
          "type": "string",
          "minLength": 1,
          "description": "The generated conversational response"
        },
        "conversation_id": {
          "type": "string",
          "description": "Unique identifier for this conversation"
        },
        "tokens_used": {
          "type": "integer",
          "minimum": 0,
          "description": "Number of tokens consumed for generation"
        },
        "finish_reason": {
          "type": "string",
          "enum": [
            "stop",
            "length",
            "timeout"
          ],
          "description": "Why generation finished"
        },
        "model_used": {
          "type": "string",
          "description": "Model that generated the response"
        },
        "generation_time_ms": {
          "type": "integer",
          "minimum": 0,
          "description": "Generation time in milliseconds"
        },
        "context_length": {
          "type": "integer",
          "minimum": 0,
          "description": "Length of context provided"
        },
        "temperature_used": {
          "type": "number",
          "minimum": 0,
          "maximum": 2,
          "description": "Temperature setting used"
        }
      },
      "required": [
        "response"
      ]
    }
  }
}