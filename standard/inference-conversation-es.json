{
  "urn": {
    "tags": {
      "op": "conversation",
      "language": "es",
      "type": "constrained",
      "in": "media:type=string;v=1;textable;scalar",
      "out": "media:type=llm-inference-output;v=1;textable;keyed"
    }
  },
  "command": "llm_inference",
  "title": "Converse in Spanish",
  "cap_description": "Conversación natural y respuestas de chat en español",
  "metadata": {},
  "media_specs": {
    "media:type=llm-inference-output;v=1;textable;keyed": {
      "media_type": "application/json",
      "profile_uri": "https://capns.org/schema/llm_inference-output",
      "schema": {
        "type": "object",
        "additionalProperties": false,
        "properties": {
          "respuesta": {
            "type": "string",
            "minLength": 1,
            "description": "La respuesta conversacional generada"
          },
          "id_conversacion": {
            "type": "string",
            "description": "Identificador único para esta conversación"
          },
          "tokens_usados": {
            "type": "integer",
            "minimum": 0,
            "description": "Número de tokens consumidos para la generación"
          },
          "razon_finalizacion": {
            "type": "string",
            "enum": [
              "stop",
              "length",
              "timeout"
            ],
            "description": "Por qué terminó la generación"
          },
          "modelo_usado": {
            "type": "string",
            "description": "Modelo que generó la respuesta"
          },
          "tiempo_generacion_ms": {
            "type": "integer",
            "minimum": 0,
            "description": "Tiempo de generación en milisegundos"
          },
          "longitud_contexto": {
            "type": "integer",
            "minimum": 0,
            "description": "Longitud del contexto proporcionado"
          },
          "temperatura_usada": {
            "type": "number",
            "minimum": 0,
            "maximum": 2,
            "description": "Configuración de temperatura utilizada"
          }
        },
        "required": [
          "respuesta"
        ]
      }
    }
  },
  "stdin": "media:type=string;v=1;textable;scalar",
  "arguments": {
    "required": [
      {
        "name": "prompt",
        "media_urn": "media:type=string;v=1;textable;scalar",
        "arg_description": "Entrada conversacional del usuario",
        "cli_flag": "--prompt",
        "position": 0,
        "validation": {
          "min_length": 1,
          "max_length": 32768
        }
      }
    ],
    "optional": [
      {
        "name": "context",
        "media_urn": "media:type=string;v=1;textable;scalar",
        "arg_description": "Contexto e historial de conversación",
        "cli_flag": "--context",
        "validation": {
          "max_length": 65536
        }
      },
      {
        "name": "max_tokens",
        "media_urn": "media:type=number;v=1;textable;numeric;scalar",
        "arg_description": "Máximo de tokens a generar",
        "cli_flag": "--max-tokens",
        "default_value": 2000,
        "validation": {
          "min": 1,
          "max": 8192
        }
      },
      {
        "name": "temperature",
        "media_urn": "media:type=number;v=1;textable;numeric;scalar",
        "arg_description": "Temperatura de muestreo para creatividad vs consistencia",
        "cli_flag": "--temperature",
        "default_value": 0.7,
        "validation": {
          "min": 0,
          "max": 2
        }
      },
      {
        "name": "system_prompt",
        "media_urn": "media:type=string;v=1;textable;scalar",
        "arg_description": "Instrucciones del sistema para comportamiento conversacional",
        "cli_flag": "--system-prompt",
        "validation": {
          "max_length": 8192
        }
      },
      {
        "name": "model_spec",
        "media_urn": "media:type=string;v=1;textable;scalar",
        "arg_description": "Model spec a usar para la conversación",
        "cli_flag": "--model-spec",
        "default_value": "hf:MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF?include=*Q8_0*,*.json&exclude=*IQ1*,*IQ2*,*IQ3*,*Q2*,*Q3*",
        "validation": {
          "pattern": ".*:.*",
          "min_length": 4,
          "max_length": 512
        }
      }
    ]
  },
  "output": {
    "media_urn": "media:type=llm-inference-output;v=1;textable;keyed",
    "output_description": "Respuesta conversacional generada con información de contexto"
  }
}