{
  "urn": {
    "tags": {
      "op": "generate_text",
      "type": "llm",
      "model_type": "mlx",
      "in": "media:mlx-model-path;textable;scalar",
      "out": "media:llm-inference-output;textable;keyed"
    }
  },
  "command": "run_inference",
  "title": "Generate Text with LLM (MLX)",
  "cap_description": "Generate text using MLX-based LLM models on Apple Silicon",
  "metadata": {},
  "media_specs": {
    "media:llm-inference-output;textable;keyed": {
      "media_type": "application/json",
      "profile_uri": "https://capns.org/schema/llm_inference-output",
      "schema": {
        "type": "object",
        "additionalProperties": false,
        "required": [
          "success",
          "generated_text",
          "model_name",
          "tokens_generated"
        ],
        "properties": {
          "success": {
            "type": "boolean",
            "description": "Whether generation succeeded"
          },
          "generated_text": {
            "type": "string",
            "description": "The generated text response"
          },
          "model_name": {
            "type": "string",
            "minLength": 1,
            "description": "Name of the model used for generation"
          },
          "tokens_generated": {
            "type": "integer",
            "minimum": 0,
            "description": "Number of tokens generated"
          },
          "generation_time_ms": {
            "type": "number",
            "minimum": 0,
            "description": "Generation time in milliseconds"
          },
          "prompt_tokens": {
            "type": "integer",
            "minimum": 0,
            "description": "Number of tokens in the prompt"
          },
          "finish_reason": {
            "type": "string",
            "enum": [
              "stop",
              "length"
            ],
            "description": "Why generation finished"
          }
        }
      }
    }
  },
  "args": [
    {
      "media_urn": "media:mlx-model-path;textable;scalar",
      "required": true,
      "sources": [
        {
          "stdin": "media:mlx-model-path;textable;scalar"
        },
        {
          "cli_flag": "--model"
        },
        {
          "position": 0
        }
      ],
      "arg_description": "Path to MLX model directory or HuggingFace model ID"
    },
    {
      "media_urn": "media:prompt-text;textable;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--prompt"
        }
      ],
      "arg_description": "Input text prompt for generation",
      "validation": {
        "max_length": 65536
      }
    },
    {
      "media_urn": "media:max-tokens;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--max-tokens"
        }
      ],
      "arg_description": "Maximum number of tokens to generate",
      "validation": {
        "max": 8192
      },
      "default_value": 512
    },
    {
      "media_urn": "media:temperature;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--temperature"
        }
      ],
      "arg_description": "Sampling temperature (0.0-2.0)",
      "default_value": 0.7
    },
    {
      "media_urn": "media:top-p;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--top-p"
        }
      ],
      "arg_description": "Top-p (nucleus) sampling parameter",
      "default_value": 0.9
    },
    {
      "media_urn": "media:repetition-penalty;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--repetition-penalty"
        }
      ],
      "arg_description": "Repetition penalty",
      "validation": {
        "min": 1
      },
      "default_value": 1.1
    },
    {
      "media_urn": "media:seed;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--seed"
        }
      ],
      "arg_description": "Random seed for reproducibility"
    },
    {
      "media_urn": "media:stream-flag;textable;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--stream"
        }
      ],
      "arg_description": "Enable streaming output",
      "default_value": false
    }
  ],
  "output": {
    "media_urn": "media:llm-inference-output;textable;keyed",
    "output_description": "Generated text with metadata including tokens and timing"
  }
}