{
  "urn": {
    "tags": {
      "op": "generate_text",
      "type": "llm",
      "format": "gguf",
      "model_type": "gguf",
      "in": "media:type=string;v=1;textable;scalar",
      "out": "media:type=llm-inference-output;v=1;textable;keyed"
    }
  },
  "command": "generate",
  "title": "Generate Text with LLM (GGUF)",
  "cap_description": "Generate text using GGUF models with llama.cpp backend",
  "metadata": {},
  "media_specs": {
    "media:type=llm-inference-output;v=1;textable;keyed": {
      "media_type": "application/json",
      "profile_uri": "https://capns.org/schema/llm_inference-output",
      "schema": {
        "type": "object",
        "additionalProperties": false,
        "required": [
          "generated_text",
          "model_name",
          "tokens_generated"
        ],
        "properties": {
          "generated_text": {
            "type": "string",
            "description": "The generated text response"
          },
          "model_name": {
            "type": "string",
            "minLength": 1,
            "description": "Name of the model used for generation"
          },
          "tokens_generated": {
            "type": "integer",
            "minimum": 0,
            "description": "Number of tokens generated"
          },
          "generation_time_ms": {
            "type": "integer",
            "minimum": 0,
            "description": "Generation time in milliseconds"
          },
          "prompt_tokens": {
            "type": "integer",
            "minimum": 0,
            "description": "Number of tokens in the prompt"
          },
          "finish_reason": {
            "type": "string",
            "enum": [
              "stop",
              "length",
              "interrupted"
            ],
            "description": "Why generation finished"
          }
        }
      }
    }
  },
  "accepts_stdin": true,
  "arguments": {
    "optional": [
      {
        "name": "prompt",
        "media_urn": "media:type=string;v=1;textable;scalar",
        "arg_description": "Input text prompt for generation",
        "cli_flag": "--prompt",
        "validation": {
          "min_length": 1,
          "max_length": 65536
        }
      },
      {
        "name": "file",
        "media_urn": "media:type=string;v=1;textable;scalar",
        "arg_description": "Path to input file containing prompt",
        "cli_flag": "--file",
        "validation": {
          "pattern": "^[^\\0]+$",
          "min_length": 1
        }
      },
      {
        "name": "model_spec",
        "media_urn": "media:type=string;v=1;textable;scalar",
        "arg_description": "GGUF model specification (HuggingFace repo or local path)",
        "cli_flag": "--model-spec",
        "validation": {
          "min_length": 1,
          "max_length": 512
        }
      },
      {
        "name": "max_tokens",
        "media_urn": "media:type=integer;v=1;textable;numeric;scalar",
        "arg_description": "Maximum number of tokens to generate",
        "cli_flag": "--max-tokens",
        "default_value": 512,
        "validation": {
          "min": 1,
          "max": 8192
        }
      },
      {
        "name": "temperature",
        "media_urn": "media:type=number;v=1;textable;numeric;scalar",
        "arg_description": "Sampling temperature (0.0-2.0)",
        "cli_flag": "--temperature",
        "default_value": 0.7,
        "validation": {
          "min": 0,
          "max": 2
        }
      },
      {
        "name": "top_k",
        "media_urn": "media:type=integer;v=1;textable;numeric;scalar",
        "arg_description": "Top-k sampling parameter",
        "cli_flag": "--top-k",
        "default_value": 40,
        "validation": {
          "min": 1,
          "max": 1000
        }
      },
      {
        "name": "top_p",
        "media_urn": "media:type=number;v=1;textable;numeric;scalar",
        "arg_description": "Top-p (nucleus) sampling parameter",
        "cli_flag": "--top-p",
        "default_value": 0.9,
        "validation": {
          "min": 0,
          "max": 1
        }
      },
      {
        "name": "min_p",
        "media_urn": "media:type=number;v=1;textable;numeric;scalar",
        "arg_description": "Min-p sampling parameter",
        "cli_flag": "--min-p",
        "default_value": 0.05,
        "validation": {
          "min": 0,
          "max": 1
        }
      },
      {
        "name": "seed",
        "media_urn": "media:type=integer;v=1;textable;numeric;scalar",
        "arg_description": "Random seed for reproducibility",
        "cli_flag": "--seed",
        "default_value": 42,
        "validation": {
          "min": 0,
          "max": 4294967295
        }
      },
      {
        "name": "max_context_length",
        "media_urn": "media:type=integer;v=1;textable;numeric;scalar",
        "arg_description": "Maximum context length",
        "cli_flag": "--max-context-length",
        "default_value": 4096,
        "validation": {
          "min": 512,
          "max": 131072
        }
      },
      {
        "name": "batch_size",
        "media_urn": "media:type=integer;v=1;textable;numeric;scalar",
        "arg_description": "Batch size for processing",
        "cli_flag": "--batch-size",
        "default_value": 2048,
        "validation": {
          "min": 1,
          "max": 8192
        }
      },
      {
        "name": "stream",
        "media_urn": "media:type=boolean;v=1;textable;scalar",
        "arg_description": "Enable streaming output",
        "cli_flag": "--stream",
        "default_value": false
      },
      {
        "name": "output",
        "media_urn": "media:type=string;v=1;textable;scalar",
        "arg_description": "Output JSON file path",
        "cli_flag": "--output",
        "validation": {
          "pattern": "^[^\\0]+$"
        }
      }
    ]
  },
  "output": {
    "media_urn": "media:type=llm-inference-output;v=1;textable;keyed",
    "output_description": "Generated text with metadata including tokens and timing"
  }
}