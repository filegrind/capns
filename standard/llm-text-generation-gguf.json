{
  "urn": {
    "tags": {
      "op": "generate_text",
      "type": "llm",
      "format": "gguf",
      "model_type": "gguf",
      "in": "media:prompt-text;textable;scalar",
      "out": "media:llm-inference-output;textable;keyed"
    }
  },
  "command": "run_inference",
  "title": "Generate Text with LLM (GGUF)",
  "cap_description": "Generate text using GGUF models with llama.cpp backend",
  "metadata": {},
  "media_specs": {
    "media:llm-inference-output;textable;keyed": {
      "media_type": "application/json",
      "profile_uri": "https://capns.org/schema/llm_inference-output",
      "schema": {
        "type": "object",
        "additionalProperties": false,
        "required": [
          "generated_text",
          "model_name",
          "tokens_generated"
        ],
        "properties": {
          "generated_text": {
            "type": "string",
            "description": "The generated text response"
          },
          "model_name": {
            "type": "string",
            "minLength": 1,
            "description": "Name of the model used for generation"
          },
          "tokens_generated": {
            "type": "integer",
            "minimum": 0,
            "description": "Number of tokens generated"
          },
          "generation_time_ms": {
            "type": "integer",
            "minimum": 0,
            "description": "Generation time in milliseconds"
          },
          "prompt_tokens": {
            "type": "integer",
            "minimum": 0,
            "description": "Number of tokens in the prompt"
          },
          "finish_reason": {
            "type": "string",
            "enum": [
              "stop",
              "length",
              "interrupted"
            ],
            "description": "Why generation finished"
          }
        }
      }
    }
  },
  "args": [
    {
      "media_urn": "media:prompt-text;textable;scalar",
      "required": false,
      "sources": [
        {
          "stdin": "media:prompt-text;textable;scalar"
        },
        {
          "cli_flag": "--prompt"
        }
      ],
      "arg_description": "Input text prompt for generation",
      "validation": {
        "max_length": 65536
      }
    },
    {
      "media_urn": "media:file-path;textable;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--file"
        }
      ],
      "arg_description": "Path to input file containing prompt"
    },
    {
      "media_urn": "media:model-spec;textable;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--model-spec"
        }
      ],
      "arg_description": "GGUF model specification (HuggingFace repo or local path)",
      "validation": {
        "max_length": 512
      }
    },
    {
      "media_urn": "media:max-tokens;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--max-tokens"
        }
      ],
      "arg_description": "Maximum number of tokens to generate",
      "validation": {
        "max": 8192
      },
      "default_value": 512
    },
    {
      "media_urn": "media:temperature;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--temperature"
        }
      ],
      "arg_description": "Sampling temperature (0.0-2.0)",
      "default_value": 0.7
    },
    {
      "media_urn": "media:top-k;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--top-k"
        }
      ],
      "arg_description": "Top-k sampling parameter",
      "default_value": 40
    },
    {
      "media_urn": "media:top-p;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--top-p"
        }
      ],
      "arg_description": "Top-p (nucleus) sampling parameter",
      "default_value": 0.9
    },
    {
      "media_urn": "media:min-p;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--min-p"
        }
      ],
      "arg_description": "Min-p sampling parameter",
      "default_value": 0.05
    },
    {
      "media_urn": "media:seed;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--seed"
        }
      ],
      "arg_description": "Random seed for reproducibility",
      "default_value": 42
    },
    {
      "media_urn": "media:max-context-length;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--max-context-length"
        }
      ],
      "arg_description": "Maximum context length",
      "validation": {
        "min": 512,
        "max": 131072
      },
      "default_value": 4096
    },
    {
      "media_urn": "media:batch-size;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--batch-size"
        }
      ],
      "arg_description": "Batch size for processing",
      "validation": {
        "max": 8192
      },
      "default_value": 2048
    },
    {
      "media_urn": "media:stream-flag;textable;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--stream"
        }
      ],
      "arg_description": "Enable streaming output",
      "default_value": false
    },
    {
      "media_urn": "media:output-path;textable;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--output"
        }
      ],
      "arg_description": "Output JSON file path"
    }
  ],
  "output": {
    "media_urn": "media:llm-inference-output;textable;keyed",
    "output_description": "Generated text with metadata including tokens and timing"
  }
}