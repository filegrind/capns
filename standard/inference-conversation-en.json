{
  "urn": {
    "tags": {
      "op": "conversation",
      "language": "en",
      "type": "constrained",
      "in": "media:type=prompt-text;v=1;textable;scalar",
      "out": "media:type=llm-inference-output;v=1;textable;keyed"
    }
  },
  "command": "llm_inference",
  "title": "Converse in English",
  "cap_description": "Natural conversation and chat responses in English",
  "metadata": {},
  "media_specs": {
    "media:type=llm-inference-output;v=1;textable;keyed": {
      "media_type": "application/json",
      "profile_uri": "https://capns.org/schema/llm_inference-output",
      "schema": {
        "type": "object",
        "additionalProperties": false,
        "properties": {
          "response": {
            "type": "string",
            "minLength": 1,
            "description": "The generated conversational response"
          },
          "conversation_id": {
            "type": "string",
            "description": "Unique identifier for this conversation"
          },
          "tokens_used": {
            "type": "integer",
            "minimum": 0,
            "description": "Number of tokens consumed for generation"
          },
          "finish_reason": {
            "type": "string",
            "enum": [
              "stop",
              "length",
              "timeout"
            ],
            "description": "Why generation finished"
          },
          "model_used": {
            "type": "string",
            "description": "Model that generated the response"
          },
          "generation_time_ms": {
            "type": "integer",
            "minimum": 0,
            "description": "Generation time in milliseconds"
          },
          "context_length": {
            "type": "integer",
            "minimum": 0,
            "description": "Length of context provided"
          },
          "temperature_used": {
            "type": "number",
            "minimum": 0,
            "maximum": 2,
            "description": "Temperature setting used"
          }
        },
        "required": [
          "response"
        ]
      }
    }
  },
  "args": [
    {
      "media_urn": "media:type=prompt-text;v=1;textable;scalar",
      "required": true,
      "sources": [
        {
          "stdin": "media:type=prompt-text;v=1;textable;scalar"
        },
        {
          "cli_flag": "--prompt"
        },
        {
          "position": 0
        }
      ],
      "arg_description": "User's conversational input",
      "validation": {
        "min_length": 1,
        "max_length": 32768
      }
    },
    {
      "media_urn": "media:type=conversation-context;v=1;textable;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--context"
        }
      ],
      "arg_description": "Conversation context and history",
      "validation": {
        "max_length": 65536
      }
    },
    {
      "media_urn": "media:type=max-tokens;v=1;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--max-tokens"
        }
      ],
      "arg_description": "Maximum tokens to generate",
      "validation": {
        "min": 1,
        "max": 8192
      },
      "default_value": 2000
    },
    {
      "media_urn": "media:type=temperature;v=1;textable;numeric;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--temperature"
        }
      ],
      "arg_description": "Sampling temperature for creativity vs consistency",
      "validation": {
        "min": 0,
        "max": 2
      },
      "default_value": 0.7
    },
    {
      "media_urn": "media:type=system-prompt;v=1;textable;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--system-prompt"
        }
      ],
      "arg_description": "System instructions for conversation behavior",
      "validation": {
        "max_length": 8192
      }
    },
    {
      "media_urn": "media:type=model-spec;v=1;textable;scalar",
      "required": false,
      "sources": [
        {
          "cli_flag": "--model-spec"
        }
      ],
      "arg_description": "Model spec to use for conversation",
      "validation": {
        "pattern": ".*:.*",
        "min_length": 4,
        "max_length": 512
      },
      "default_value": "hf:MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF?include=*Q8_0*,*.json&exclude=*IQ1*,*IQ2*,*IQ3*,*Q2*,*Q3*"
    }
  ],
  "output": {
    "media_urn": "media:type=llm-inference-output;v=1;textable;keyed",
    "output_description": "Generated conversational response with context information"
  }
}